{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranaysood/anaconda3/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainImages size:(60000, 28, 28)\n",
      "trainLabels size:(60000, 1)\n",
      "testImages size:(10000, 28, 28)\n",
      "testLabels size:(10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranaysood/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:14: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "\n",
    "trainImages = read_idx('train-images-idx3-ubyte')\n",
    "trainLabels = read_idx('train-labels-idx1-ubyte')\n",
    "testImages = read_idx('t10k-images-idx3-ubyte')\n",
    "testLabels = read_idx('t10k-labels-idx1-ubyte')\n",
    "\n",
    "train_label = trainLabels.reshape(trainImages.shape[0],1)\n",
    "test_label = testLabels.reshape(testImages.shape[0],1)\n",
    "\n",
    "print('trainImages size:' + str(trainImages.shape))\n",
    "print('trainLabels size:' + str(train_label.shape))\n",
    "print('testImages size:' + str(testImages.shape))\n",
    "print('testLabels size:' + str(test_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_image size:(60000, 28, 28, 1)\n",
      "train_label size:(60000, 10)\n",
      "test_image size:(10000, 28, 28, 1)\n",
      "test_label size:(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def one_hot_matrix(Y_onehot,C):\n",
    "    Y_onehot = np.eye(C)[Y_onehot.reshape(-1)].T\n",
    "    return Y_onehot\n",
    "\n",
    "train_label_one_hot = one_hot_matrix(train_label,10).T\n",
    "test_label_one_hot = one_hot_matrix(test_label,10).T\n",
    "train_data = trainImages/255\n",
    "test_data = testImages/255\n",
    "train_data = train_data.reshape(train_data.shape[0],28,28,1)\n",
    "test_data = test_data.reshape(test_data.shape[0],28,28,1)\n",
    "\n",
    "print('train_image size:' + str(train_data.shape))\n",
    "print('train_label size:' + str(train_label_one_hot.shape))\n",
    "print('test_image size:' + str(test_data.shape))\n",
    "print('test_label size:' + str(test_label_one_hot.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Placeholders\n",
    "\n",
    "'''\n",
    "Arguments:\n",
    "n_H -- scalar, height of an input image\n",
    "n_W -- scalar, width of an input image\n",
    "n_C -- scalar, number of channels of the input\n",
    "n_y -- scalar, number of classes\n",
    "   \n",
    "'''\n",
    "def create_placeholder(n_H,n_W,n_C,n_y):\n",
    "    X = tf.placeholder(tf.float32, [None, n_H, n_W, n_C])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_y])\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    #number of training examples\n",
    "    m = X.shape[0]                  \n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #Step 1: Shuffle(X,Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "    \n",
    "    #Step 2: Partition (shuffled_X, shuffled_Y). (Not including the end case)\n",
    "    #number of mini batches of size mini_batch_size in your partitionning\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    for i in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[i * mini_batch_size : i * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[i * mini_batch_size : i * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    #Step 3: Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Parameters\n",
    "def initialize_parameters(n_size):\n",
    "    \n",
    "    tf.set_random_seed(1)               \n",
    "        \n",
    "    W1 = tf.get_variable(\"W1\", [3, 3, 1, n_size], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", [3, 3, n_size, n_size], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W3 = tf.get_variable(\"W3\", [3, 3, n_size, n_size], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W4 = tf.get_variable(\"W4\", [3, 3, n_size, n_size], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W5 = tf.get_variable(\"W5\", [3, 3, n_size, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "   \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2,\n",
    "                  \"W3\": W3,\n",
    "                  \"W4\": W4,\n",
    "                  \"W5\": W5}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Propagation \n",
    "def forward_propagation(X,parameters):\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    W4 = parameters['W4']\n",
    "    W5 = parameters['W5']\n",
    "\n",
    "    #Layer 1\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A1 = tf.nn.leaky_relu(Z1,alpha=0.2)\n",
    "    \n",
    "    #Layer 2\n",
    "    Z2 = tf.nn.atrous_conv2d(A1,W2, rate=2, padding = 'SAME')\n",
    "    A2 = tf.nn.leaky_relu(Z2,alpha=0.2)\n",
    "    \n",
    "    #Layer 3\n",
    "    Z3 = tf.nn.atrous_conv2d(A2,W3, rate=4, padding = 'SAME')\n",
    "    A3 = tf.nn.leaky_relu(Z3,alpha=0.2)\n",
    "    \n",
    "    #Layer 4\n",
    "    Z4 = tf.nn.atrous_conv2d(A3,W4, rate=8, padding = 'SAME')\n",
    "    A4 = tf.nn.leaky_relu(Z4,alpha=0.2)\n",
    "    \n",
    "    #Layer 5\n",
    "    Z5 = tf.nn.conv2d(A4,W5, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A5 = tf.nn.leaky_relu(Z5,alpha=0.2)   \n",
    "    \n",
    "    #Layer 6 Global Average Pool\n",
    "    Z6=  tf.reduce_mean(A5, axis=[1,2])\n",
    "    \n",
    "    #Flatten the CAN output so that we can connect it with fully connected layers\n",
    "    Z7 = tf.contrib.layers.flatten(Z6)\n",
    "    #Z7 = tf.contrib.layers.fully_connected(A6, 10, activation_fn=tf.nn.softmax)\n",
    "    \n",
    "    return Z7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Cost\n",
    "def compute_cost(Z7,Y):\n",
    "    \n",
    "    logits = Z7\n",
    "    labels = Y\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test,n_size,learning_rate = 0.005,\n",
    "          num_epochs = 10, minibatch_size = 64, print_cost = True):\n",
    "\n",
    "    \n",
    "    #Rerun model without overwriting tf variables\n",
    "    ops.reset_default_graph()                         \n",
    "    tf.set_random_seed(1)                             \n",
    "    seed = 3                                          \n",
    "    (m, n_H, n_W, n_C) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = [] \n",
    "    \n",
    "    X,Y = create_placeholder(n_H,n_W,n_C,n_y)\n",
    "    \n",
    "    parameters = initialize_parameters(n_size)\n",
    "    \n",
    "    #Forward propagation: Build the forward propagation\n",
    "    Z7 = forward_propagation(X,parameters)\n",
    "    \n",
    "    #Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z7, Y)\n",
    "    \n",
    "    #Backpropagation: Descent of Gradient Usine AdamOptimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #Session to compute tensorflow graph\n",
    "        sess.run(init)\n",
    "        \n",
    "        #Training Loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            #Define a cost related to an mini_batch\n",
    "            minibatch_cost = 0 \n",
    "            #Number of minibatches of size minibatch_size in the train set\n",
    "            num_minibatches = int(m / minibatch_size) \n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                #The line that runs the graph on a minibatch.\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "                \n",
    "                #Total epoch cost for all minibatches combined\n",
    "                minibatch_cost += temp_cost / num_minibatches \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "\n",
    "        #Calculate the correct predictions\n",
    "        #Returns the index with the largest value \n",
    "        correct_prediction = tf.equal(tf.argmax(Z7,1), tf.argmax(Y,1)) \n",
    "\n",
    "        #Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=\"float\"))\n",
    "        \n",
    "        \n",
    "        test_accuracy = np.zeros(5)\n",
    "        \n",
    "        for i in range(5):\n",
    "            \n",
    "            test_accuracy[i] = accuracy.eval({X: X_test[i*2000:(i+1)*2000-1,:,:,:], Y: Y_test[i*2000:(i+1)*2000-1,:]})\n",
    "            \n",
    "        test_accuracy1 = np.mean(test_accuracy)\n",
    "        \n",
    "        print(\"No. of channels=%d, Test Accuracy:%.2f%%\" % (n_size, test_accuracy1 * 100))\n",
    "\n",
    "        train_accuracy=np.zeros(30)\n",
    "        \n",
    "        for j in range(30):\n",
    "            \n",
    "            train_accuracy[j] = accuracy.eval({X: X_train[j*2000:(j+1)*2000-1,:,:,:], Y: Y_train[j*2000:(j+1)*2000-1,:]})\n",
    "        \n",
    "        train_accuracy1 = np.mean(train_accuracy)\n",
    "        \n",
    "        print(\"No. of channels=%d, Train Accuracy:%.2f%%\" % (n_size, train_accuracy1 * 100))\n",
    "        \n",
    "        return train_accuracy1,test_accuracy1,parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-7f8267c91fb5>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Cost after epoch 0: 0.285695\n",
      "Cost after epoch 1: 0.073783\n",
      "Cost after epoch 2: 0.056033\n",
      "Cost after epoch 3: 0.044909\n",
      "Cost after epoch 4: 0.042441\n",
      "Cost after epoch 5: 0.037624\n",
      "Cost after epoch 6: 0.035964\n",
      "Cost after epoch 7: 0.032693\n",
      "Cost after epoch 8: 0.034375\n",
      "Cost after epoch 9: 0.026191\n",
      "Parameters have been trained!\n",
      "No. of channels=32, Test Accuracy:99.24%\n",
      "No. of channels=32, Train Accuracy:99.46%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy1, test_accuracy1, parameters = model(train_data, train_label_one_hot, test_data, test_label_one_hot,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
